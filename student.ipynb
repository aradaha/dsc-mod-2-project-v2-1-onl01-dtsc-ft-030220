{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project 2\n",
    "\n",
    "\n",
    "* Student name: Reuben Hough\n",
    "* Student pace: full time\n",
    "* Scheduled project review date/time: 7/20/20\n",
    "* Instructor name: Ahbineet\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science King County Housing Price\n",
    "\n",
    "In this project, I'm going to get the data, clean it, get useful information from it, model the data with linear regression, then interpret the results so they will be useful. The first step is importing libraries that I will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FuncFormatter\n",
    "from glob import glob\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder, Normalizer, MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, cross_validate\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, make_scorer, auc, accuracy_score, roc_curve\n",
    "from sklearn.svm import LinearSVC, SVR\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('kc_house_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Observation and Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly null values in the data that need to be dealt with before any kind of assesment is done, but first I will find out what they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values exist in the \"waterfront\", \"yr_renovated\" and \"view\" categories, with \"?\" appearing in the data for \"sq_basement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['waterfront'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfront refers to whether the property is found on the waterfront \"1\" or not, \"0\". With the vast majority of properties not on the waterfront, I believe it is warranted to replace null values with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df['yr_renovated'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be inappropriate to replace the null values in the year renovated category with any value other than zero. Since again the vast majority of houses have not been renovated (Indicated by zero), I beleive it would be appropriate to replace null values with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Null values in the waterfront and yr_renovated columns are replaced with zero as stated\n",
    "df.waterfront.replace(np.NaN, 0.0, inplace=True)\n",
    "df.yr_renovated.replace(np.NaN, 0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yr_renovated is turned into integer value\n",
    "df.yr_renovated = df.yr_renovated.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've found conflicting data on what \"view\" represents. It may be the number of times a house has been viewed, or an assessment of the view from the house. Either way, I will not be using this because both are somewhat arbitrary, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['view'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing basement values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are question marks in the data; thankfully it can all be replaced by subtracting the \"above\" square footage from total so there is no guessing involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sqft_basement = (df.sqft_living - df.sqft_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Sale Date more useable\n",
    "df['Year'] = df['date'].map(lambda x: '{}'.format(x[-4:].replace('/', '')))\n",
    "df['Month'] = df['date'].map(lambda x: '{}'.format(x[:2].replace('/', '')))\n",
    "df['Day'] = df['date'].map(lambda x: '{}'.format(x[-7:-5].replace('/', '')))\n",
    "df['Day'] = df['Day'].map(lambda x: int(x))\n",
    "df['Month'] = df['Month'].map(lambda x: int(x))\n",
    "df['Year'] = df['Year'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function that turns the new date columns into a julian date\n",
    "def julian(a,b,c):\n",
    "    w=[]\n",
    "    for x,y,z in zip(a,b,c):\n",
    "         w.append(pd.Timestamp(year = x,  month = y, day = z).to_julian_date())\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Turning the date into a Julian date to make it more useable\n",
    "df['Julian'] = julian(df['Year'], df['Month'], df['Day'])\n",
    "df.sort_values(by='Julian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data for any obvious errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 bedrooms is almost certainly an error based on the square footage; it's probably meant to be 3, but I'm uncertain so it will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([15856])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are none\n",
    "df.duplicated(keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a new catergory \"Seattle\" That contains all zip codes within the greater Seattle area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list found on seattlearea.com/zip-codes/\n",
    "seattle = [98003, 98005, 98033, 98037, 98040, 98052, 98055, 98101, 98101, 98102,\n",
    "            98103, 98103, 98103, 98104, 98104, 98105, 98105, 98107, 98109,\n",
    "            98109, 98110, 98110, 98116, 98116, 98118, 98121, 98125, 98144, 98199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating category with a 1 if in Seattle and 0 if not\n",
    "df['Seattle'] = df['zipcode'].apply(lambda x: 1 if x in seattle else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Month by Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts december next to other winter months\n",
    "forbin = df['Month'].replace(12,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthbins = [0,3,6,9,11]\n",
    "seasonlabels = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "binmonth = pd.cut(forbin, monthbins, labels = seasonlabels, include_lowest = True).cat.as_unordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binmonth.value_counts().plot(kind='bar',color='gray')\n",
    "plt.ylabel('Houses Sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'] = binmonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating \"basement\" as a category for houses with a basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['basement'] = df['sqft_basement'].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between size of a house and lot and the size of its neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(df['sqft_living15'], df['sqft_living'], alpha = 0.3);\n",
    "ax1.set_title('Living space in ft^2');\n",
    "ax1.set_xlabel('15 nearest neighbours area');\n",
    "ax1.set_ylabel('House area');\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.scatter(df['sqft_lot15'], df['sqft_lot'], alpha = 0.3);\n",
    "ax1.set_title('Lot space in ft^2');\n",
    "ax1.set_xlabel('15 nearest neighbours area');\n",
    "ax1.set_ylabel('House area');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Correlations in the Data to Eliminate Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are subplots comparing price to several different factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables = ['floors', 'bedrooms', 'bathrooms','waterfront','condition','grade','Month','Seattle','season','basement']\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14,20))\n",
    "\n",
    "for column, ax in zip(variables, axes.flatten()):\n",
    "    (df.groupby(column).mean()['price'].sort_values().plot.bar(ax=ax))                \n",
    "    \n",
    "    ax.set_title(column)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transforming the price may the data easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['logprice'] = np.log(df['price'])\n",
    "df[['price', 'logprice']].hist(figsize=(12,5),bins=30,color = \"skyblue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14,20))\n",
    "\n",
    "for column, ax in zip(variables, axes.flatten()):\n",
    "    (df.groupby(column).mean()['logprice'].sort_values().plot.bar(ax=ax))                \n",
    "    \n",
    "    ax.set_title(column)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables = ['floors', 'bedrooms', 'bathrooms', 'sqft_living','sqft_lot', 'sqft_above', 'waterfront','condition','grade','sqft_living15','sqft_lot15','Month','Seattle','season','basement']\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df[variables].corr(), cmap='coolwarm', annot=True, linewidths=.5, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things are already noticable. There is minor correlation between bathrooms, grade, and bedrooms, which is expected. Living in the Seattle area, having a basement, and having more floors are good indicators of price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also sqft_above seems to have strong correlation with multiple columns so it should be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sqft_living'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning month by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts december next to other winter months\n",
    "forbin = df['Month'].replace(12,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthbins = [0,3,6,9,11]\n",
    "seasonlabels = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "binmonth = pd.cut(forbin, monthbins, labels = seasonlabels, include_lowest = True).cat.as_unordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binmonth.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring clearly has the most houses purchased, and fall is a very unpopular time to buy a house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for and removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers will skew the data, so getting rid of a few outliers should help in that regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[df['sqft_lot15']>500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['price']>6000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[df['sqft_lot']>1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=[9509,13177,20010,3846,7105,9059,1687,7489,7610,16940], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to examine a histogram and Q-Q plot, then plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plot(df, var):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[var].hist(bins=30)\n",
    "    plt.title(var)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[var], dist=\"norm\", plot=plt)\n",
    "    plt.title(var)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['floors', 'bedrooms', 'bathrooms', 'sqft_above','sqft_lot','waterfront','condition','grade','sqft_living15','sqft_lot15','Month','Seattle','basement']\n",
    "for col in df[variables]:\n",
    "    diagnostic_plot(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Linear Regression to Predict Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns that are no longer useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'date', 'lat', 'long'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dummy variables from categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['condition','grade','zipcode','basement','season','waterfront','Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditiondum = pd.get_dummies(df[\"condition\"], prefix=\"condition\", drop_first=True)\n",
    "gradedum = pd.get_dummies(df[\"grade\"], prefix=\"grade\", drop_first=True)\n",
    "zipcodedum = pd.get_dummies(df[\"zipcode\"], prefix=\"zipcode\", drop_first=True)\n",
    "basementdum = pd.get_dummies(df[\"basement\"], prefix=\"basement\", drop_first=True)\n",
    "seasondum = pd.get_dummies(df[\"season\"], prefix=\"season\", drop_first=True)\n",
    "waterfrontdum = pd.get_dummies(df[\"waterfront\"], prefix=\"waterfront\", drop_first=True)\n",
    "monthdum = pd.get_dummies(df[\"Month\"], prefix=\"month\", drop_first=True)\n",
    "olddf = df\n",
    "\n",
    "df.drop(categoricals, axis=1, inplace = True)\n",
    "df = pd.concat([df, conditiondum, gradedum, zipcodedum, basementdum, seasondum, waterfrontdum, monthdum], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This funtion shows a large amount of subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noprice = df.drop(['price', 'logprice'], axis = 1)\n",
    "\n",
    "plt.figure(1, figsize=(20,20))\n",
    "        \n",
    "def multi_scatter_plot(x):\n",
    "    sns.scatterplot(x, y=\"logprice\", data=df) #iterating over dataframe minus price columns, adding subplots\n",
    "\n",
    "for index, col in enumerate(noprice.columns, start=1):\n",
    "\n",
    "    plt.subplot(11, 11,index)\n",
    "\n",
    "    multi_scatter_plot(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(olddf.corr(), cmap='coolwarm', annot=True, linewidths=.5, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is not too significant (under .75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data makes it easier to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqft_above'] = np.log(df['sqft_above'])\n",
    "df['sqft_above'].plot.hist(density=True )\n",
    "df['sqft_above'].plot.kde(label='sqft_above')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqft_living15'] = np.log(df['sqft_living15'])\n",
    "df['sqft_living15'].plot.hist(density=True )\n",
    "df['sqft_living15'].plot.kde(label='sqft_living15')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqft_lot'] = np.log(df['sqft_lot'])\n",
    "df['sqft_lot'].plot.hist(density=True )\n",
    "df['sqft_lot'].plot.kde(label='sflot_log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqft_lot15'] = np.log(df['sqft_lot15'])\n",
    "df['sqft_lot15'].plot.hist(density=True )\n",
    "df['sqft_lot15'].plot.kde(label='sqft_lot15')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['yr_built'] = np.log(df['yr_built'])\n",
    "df['yr_built'].plot.hist(density=True )\n",
    "df['yr_built'].plot.kde(label='yr_built')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Regression Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train test split is used to validate our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['logprice'], axis=1)\n",
    "y = df['logprice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "x_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model_fit = sm.OLS(y_train, x_train).fit()\n",
    "results_df = pd.concat([x_train, y_train], axis=1)\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "#fitting model on training data\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "#predicting on the training data\n",
    "y_pred=lr.predict(X_train)\n",
    "\n",
    "#calculating score on the linear regression model on the training set\n",
    "r_score=lr.score(X_train,y_train)\n",
    "r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r score is very good. I don't believe it's a result of overfitting because I haven't used polynomial regression. It may be due to the way the log of the price normalized so well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# waterfront caused an error, possibly related to its name'1.0', so it needs to be dropped\n",
    "df.drop(['waterfront_1.0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm getting the rsquared values so that I can decide which variables are the best predictors\n",
    "col_names = df.describe().columns.drop(['logprice'])\n",
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]]\n",
    "for idx, val in enumerate(col_names):\n",
    "    print (\"Housing: Price~\" + val)\n",
    "    print (\"------------------------------\")\n",
    "\n",
    "    f = 'price~' + val\n",
    "    model = ols(formula=f, data=df).fit()\n",
    "    X_new = pd.DataFrame({val: [df[val].min(), df[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ])\n",
    "    # param[0] is slope and param[1] is intercept\n",
    "    print(results[idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathrooms, sq_above, sq_basement, sqft_living15, grade_10, grade_11, have a relatively high r squared value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year, Julian, condition_3, grade_8, zipcode_98011, zipcode_98024, zipcode_98034, zipcode_98065, zipcode_98070, zipcode_98072, zipcode_98107, zipcode_98136, season_Summer, season_Autumn, month_3, month_5,month_7,month_8, month_9, month_10, month_11, and month_12 have p values higher than the acceptable .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Year', 'Julian', 'condition_3', 'grade_8', 'zipcode_98011', 'zipcode_98024', 'zipcode_98034', 'zipcode_98065', 'zipcode_98070', 'zipcode_98072', 'zipcode_98107', 'zipcode_98136', 'season_Summer', 'season_Autumn', 'month_3', 'month_5','month_7','month_8', 'month_9', 'month_10', 'month_11', 'month_12'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_names = df.describe().columns.drop(['logprice'])\n",
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]]\n",
    "for idx, val in enumerate(col_names):\n",
    "    print (\"Housing: Price~\" + val)\n",
    "    print (\"------------------------------\")\n",
    "\n",
    "    f = 'price~' + val\n",
    "    model = ols(formula=f, data=df).fit()\n",
    "    X_new = pd.DataFrame({val: [df[val].min(), df[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ])\n",
    "    # param[0] is slope and param[1] is intercept\n",
    "    print(results[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'logprice ~ {}'.format(df.column)\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rsquared value is far from useful, so I'm going to pick the top variables based on p-values. I believe it would be more broadly useful from a business perspective to include different sorts of variables; in this case the top two months, the top two addition related variables, and the top two zipcodes based on rsquared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['logprice','sqft_above', 'bathrooms', 'zipcode_98004', 'zipcode_98112', 'month_2', 'month_4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# refitting the model\n",
    "f = 'logprice ~ sqft_above + bathrooms + zipcode_98004 + zipcode_98112 + month_2 + month_4'\n",
    "model = ols(formula=f, data=new_df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for normality. Looks very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.predict(X_test)\n",
    "plt.scatter(y_test,predictions)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was checked for the regression assumptions and appeared to pass. The r_squared value is somewhat low, however it the predictions compared to actual prices look reasonably linear."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
